{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio Final: Descenso de Gradiente para Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "En este laboratorio, usted:\n",
    "- automatizará el proceso de optimización de $w$ y $b$ usando descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas\n",
    "En este laboratorio utilizaremos:\n",
    "- NumPy, una biblioteca popular para computación científica\n",
    "- Matplotlib, una biblioteca popular para graficar datos\n",
    "- Rutinas de graficación en el archivo lab_utils.py en el directorio local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43512\\2395164312.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./deeplearning.mplstyle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mejerc_utils_corhuila\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplt_house_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplt_contour_wgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplt_divergence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplt_gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "from ejerc_utils_corhuila import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients\n",
    "# Se importan las funciones y librerías necesarias para el laboratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2\"></a>\n",
    "# Enunciado del problema\n",
    "\n",
    "Utilicemos los mismos dos puntos de datos que antes: una casa de 1000 metros cuadrados se vendió por \\$300,000 y una casa de 2000 metros cuadrados se vendió por \\$500,000.\n",
    "\n",
    "| Tamaño (1000 m²)     | Precio (miles de dólares) |\n",
    "| ---------------------- | ------------------------ |\n",
    "| 1                      | 300                      |\n",
    "| 2                      | 500                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar nuestro conjunto de datos\n",
    "x_train = np.array([1.0, 2.0])   #características\n",
    "y_train = np.array([300.0, 500.0])   #valor objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.0.1\"></a>\n",
    "### Calcular_Costo\n",
    "Esto se desarrolló en el ejercicio anterior. Lo necesitaremos nuevamente aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular el costo\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.1\"></a>\n",
    "## Resumen del descenso de gradiente\n",
    "Hasta ahora en este curso, has desarrollado un modelo lineal que predice $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "En la regresión lineal, utilizas datos de entrenamiento de entrada para ajustar los parámetros $w$,$b$ minimizando una medida del error entre nuestras predicciones $f_{w,b}(x^{(i)})$ y los datos reales $y^{(i)}$. Esta medida se llama $costo$, $J(w,b)$. Durante el entrenamiento, mides el costo sobre todas nuestras muestras de entrenamiento $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la clase, el *descenso de gradiente* se describió como:\n",
    "\n",
    "$$\\begin{align*} \\text{repetir}&\\text{ hasta convergencia:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "donde los parámetros $w$, $b$ se actualizan simultáneamente.  \n",
    "El gradiente se define como:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aquí *simultáneamente* significa que calculas las derivadas parciales para todos los parámetros antes de actualizar cualquiera de los parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.2\"></a>\n",
    "## Implementar Descenso de Gradiente\n",
    "Implementarás el algoritmo de descenso de gradiente para una característica. Necesitarás tres funciones:\n",
    "- `compute_gradient` que implementa las ecuaciones (4) y (5) anteriores\n",
    "- `compute_cost` que implementa la ecuación (2) anterior (código del laboratorio previo)\n",
    "- `gradient_descent`, que utiliza compute_gradient y compute_cost\n",
    "\n",
    "Convenciones:\n",
    "- El nombre de las variables de python que contienen derivadas parciales sigue este patrón, $\\frac{\\partial J(w,b)}{\\partial b}$ será `dj_db`.\n",
    "- w.r.t significa \"con respecto a\", como en derivada parcial de $J(wb)$ con respecto a $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.3\"></a>\n",
    "### compute_gradient\n",
    "<a name='ex-01'></a>\n",
    "`compute_gradient` implementa (4) y (5) anteriores y retorna $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. Los comentarios incrustados describen las operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula el gradiente para regresión lineal \n",
    "    Argumentos:\n",
    "      x (ndarray (m,)): Datos, m ejemplos \n",
    "      y (ndarray (m,)): valores objetivo\n",
    "      w,b (scalar)    : parámetros del modelo  \n",
    "    Returns\n",
    "      dj_dw (scalar): El gradiente del costo respecto a w\n",
    "      dj_db (scalar): El gradiente del costo respecto a b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Número de ejemplos de entrenamiento\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las clases se explicaron cómo el descenso por gradiente utiliza la derivada parcial del costo con respecto a un parámetro en un punto para actualizar dicho parámetro.  \n",
    "Vamos a usar nuestra función `compute_gradient` para encontrar y graficar algunas derivadas parciales de nuestra función de costo con respecto a uno de los parámetros, $w_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba, la gráfica de la izquierda muestra $\\frac{\\partial J(w,b)}{\\partial w}$ o la pendiente de la curva de costo respecto a $w$ en tres puntos. En el lado derecho de la gráfica, la derivada es positiva, mientras que en el lado izquierdo es negativa. Debido a la 'forma de tazón', las derivadas siempre guiarán el descenso de gradiente hacia el fondo donde el gradiente es cero.\n",
    " \n",
    "La gráfica de la izquierda tiene $b=100$ fijo. El descenso de gradiente utilizará tanto $\\frac{\\partial J(w,b)}{\\partial w}$ como $\\frac{\\partial J(w,b)}{\\partial b}$ para actualizar los parámetros. El 'gráfico de flechas' (quiver) a la derecha proporciona una forma de visualizar el gradiente de ambos parámetros. El tamaño de las flechas refleja la magnitud del gradiente en ese punto. La dirección y pendiente de la flecha reflejan la razón entre $\\frac{\\partial J(w,b)}{\\partial w}$ y $\\frac{\\partial J(w,b)}{\\partial b}$ en ese punto.\n",
    "Nota que el gradiente apunta *lejos* del mínimo. Revisa la ecuación (3) anterior. El gradiente escalado se *resta* del valor actual de $w$ o $b$. Esto mueve el parámetro en una dirección que reducirá el costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.5\"></a>\n",
    "### Descenso de Gradiente\n",
    "Ahora que se pueden calcular los gradientes, el descenso de gradiente, descrito en la ecuación (3) anterior, puede implementarse abajo en `gradient_descent`. Los detalles de la implementación se describen en los comentarios. A continuación, utilizarás esta función para encontrar los valores óptimos de $w$ y $b$ en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Realiza descenso de gradiente para ajustar w,b. Actualiza w,b tomando \n",
    "    num_iters pasos de gradiente con tasa de aprendizaje alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Datos, m ejemplos \n",
    "      y (ndarray (m,))  : valores objetivo\n",
    "      w_in,b_in (escalar): valores iniciales de los parámetros del modelo  \n",
    "      alpha (float):     Tasa de aprendizaje\n",
    "      num_iters (int):   número de iteraciones para ejecutar descenso de gradiente\n",
    "      cost_function:     función para calcular el costo\n",
    "      gradient_function: función para calcular el gradiente\n",
    "      \n",
    "    Returns:\n",
    "      w (escalar): Valor actualizado del parámetro después de ejecutar descenso de gradiente\n",
    "      b (escalar): Valor actualizado del parámetro después de ejecutar descenso de gradiente\n",
    "      J_history (List): Historial de valores de costo\n",
    "      p_history (list): Historial de parámetros [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    # Un arreglo para guardar el costo J y los w en cada iteración, principalmente para graficar después\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calcular el gradiente y actualizar los parámetros usando gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Actualizar parámetros usando la ecuación (3) anterior\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Guardar el costo J en cada iteración\n",
    "        if i<100000:      # prevenir agotamiento de recursos \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Imprimir el costo cada 10 veces o tantas iteraciones si < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteración {i:4}: Costo {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #retorna w y el historial de J,w para graficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializar parámetros\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# algunos ajustes para descenso de gradiente\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# ejecutar descenso de gradiente\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) encontrados por descenso de gradiente: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tómate un momento y observa algunas características del proceso de descenso de gradiente impreso arriba.  \n",
    "\n",
    "- El costo inicia grande y disminuye rápidamente como se describe en la diapositiva de la clase.\n",
    "- Las derivadas parciales, `dj_dw` y `dj_db`, también se hacen más pequeñas, rápidamente al principio y luego más lentamente. Como se muestra en el diagrama de la clase, a medida que el proceso se acerca al 'fondo del tazón', el progreso es más lento debido al menor valor de la derivada en ese punto.\n",
    "- El progreso se ralentiza aunque la tasa de aprendizaje, alpha, permanece fija."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costo versus iteraciones del descenso de gradiente\n",
    "Una gráfica de costo versus iteraciones es una medida útil del progreso en el descenso de gradiente. El costo siempre debe disminuir en ejecuciones exitosas. El cambio en el costo es tan rápido al inicio, que es útil graficar el descenso inicial en una escala diferente al descenso final. En las gráficas de abajo, observa la escala del costo en los ejes y el paso de iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar costo versus iteración  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Costo vs. iteración (inicio)\");  ax2.set_title(\"Costo vs. iteración (final)\")\n",
    "ax1.set_ylabel('Costo')            ;  ax2.set_ylabel('Costo') \n",
    "ax1.set_xlabel('paso de iteración')  ;  ax2.set_xlabel('paso de iteración') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones\n",
    "Ahora que has encontrado los valores óptimos para los parámetros $w$ y $b$, puedes usar el modelo para predecir valores de viviendas basados en los parámetros aprendidos. Como era de esperarse, los valores predichos son casi iguales a los valores de entrenamiento para las mismas viviendas. Además, el valor no incluido en el entrenamiento está en línea con el valor esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicción para casa de 1000 pies²: {w_final*1.0 + b_final:0.1f} mil dólares\")\n",
    "print(f\"Predicción para casa de 1200 pies²: {w_final*1.2 + b_final:0.1f} mil dólares\")\n",
    "print(f\"Predicción para casa de 2000 pies²: {w_final*2.0 + b_final:0.1f} mil dólares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.6\"></a>\n",
    "## Graficando\n",
    "Puedes mostrar el progreso del descenso de gradiente durante su ejecución graficando el costo sobre las iteraciones en un gráfico de contorno de cost(w,b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 6))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax)\n",
    "# Se grafica el contorno del costo y la trayectoria del descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba, el gráfico de contorno muestra el $costo(w,b)$ sobre un rango de $w$ y $b$. Los niveles de costo están representados por los anillos. Sobrepuestos, usando flechas rojas, está la trayectoria del descenso de gradiente. Algunas cosas a notar:\n",
    "- La trayectoria avanza de manera constante (monótona) hacia su objetivo.\n",
    "- Los pasos iniciales son mucho más grandes que los pasos cerca del objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acercando la vista (zoom)**, podemos ver los pasos finales del descenso de gradiente. Observa que la distancia entre los pasos disminuye a medida que el gradiente se acerca a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],\n",
    "            contours=[1,5,10,20],resolution=0.5)\n",
    "# Se grafica un acercamiento para ver los últimos pasos del descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.7.1\"></a>\n",
    "### Tasa de aprendizaje aumentada\n",
    "\n",
    "En la clase, se discutió el valor adecuado de la tasa de aprendizaje, $\\alpha$ en la ecuación(3). Mientras más grande sea $\\alpha$, más rápido convergerá el descenso de gradiente a una solución. Pero, si es demasiado grande, el descenso de gradiente diverge. Arriba tienes un ejemplo de una solución que converge correctamente.\n",
    "\n",
    "Probemos aumentar el valor de $\\alpha$ y ver qué sucede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializar parámetros\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# establecer alpha a un valor grande\n",
    "iterations = 10\n",
    "tmp_alpha = 8.0e-1\n",
    "# ejecutar descenso de gradiente\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba, $w$ y $b$ están rebotando entre valores positivos y negativos, y el valor absoluto aumenta en cada iteración. Además, en cada iteración $\\frac{\\partial J(w,b)}{\\partial w}$ cambia de signo y el costo está aumentando en vez de disminuir. Esto es una clara señal de que la *tasa de aprendizaje es demasiado grande* y la solución está divergiendo. \n",
    "Visualicemos esto con una gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_divergence(p_hist, J_hist,x_train, y_train)\n",
    "plt.show()\n",
    "# Se grafica la divergencia del descenso de gradiente con tasa de aprendizaje alta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba, la gráfica de la izquierda muestra la progresión de $w$ durante los primeros pasos del descenso de gradiente. $w$ oscila de positivo a negativo y el costo crece rápidamente. El descenso de gradiente está operando sobre ambos, $w$ y $b$ simultáneamente, por lo que se necesita la gráfica 3D de la derecha para ver el panorama completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Felicidades!\n",
    "En este ejercicio usted:\n",
    "- profundizó en los detalles del descenso de gradiente para una sola variable.\n",
    "- desarrolló una rutina para calcular el gradiente\n",
    "- visualizó qué es el gradiente\n",
    "- completó una rutina de descenso de gradiente\n",
    "- utilizó descenso de gradiente para encontrar parámetros\n",
    "- examinó el impacto de ajustar la tasa de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "astropy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
