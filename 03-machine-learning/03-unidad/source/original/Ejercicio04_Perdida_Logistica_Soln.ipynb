{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    },
    "toc-showcode": true
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "12b1e4eb-32d2-4b93-8cb7-eba5d99e228a",
      "cell_type": "markdown",
      "source": "# Ejercicio: Pérdida Logística\n\n## Objetivos\nEn este ejercicio, vas a:\n- examinar la implementación y utilizar la función de pérdida para regresión logística.",
      "metadata": {}
    },
    {
      "id": "de3e044a-8fcb-41fd-9a92-76e439904497",
      "cell_type": "code",
      "source": "import numpy as np\n#%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\nfrom plt_logistic_loss import soup_bowl, plt_logistic_squared_error\nplt.style.use('./deeplearning.mplstyle')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0d8b8645-1da5-4fd7-8332-8849791a391f",
      "cell_type": "markdown",
      "source": "## Conjunto de datos\nComencemos con el mismo conjunto de datos que se usó en el ejercicio de frontera de decisión.",
      "metadata": {}
    },
    {
      "id": "685a3e8f-86d7-425f-8c3a-badfd272debb",
      "cell_type": "markdown",
      "source": "Usaremos una función auxiliar para graficar estos datos. Los puntos de datos con etiqueta $y=1$ se muestran como cruces rojas, mientras que los puntos con $y=0$ se muestran como círculos azules.",
      "metadata": {}
    },
    {
      "id": "ee8a3d28-e472-46a3-aa6c-438b9d864b38",
      "cell_type": "code",
      "source": "soup_bowl()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a378dfde-d653-40a6-9d81-c38e3518a736",
      "cell_type": "markdown",
      "source": "## Función de pérdida\n\nEn un ejercicio anterior, desarrollaste la función de *pérdida logística*. Recuerda, la pérdida se define para un solo ejemplo. Aquí combinas las pérdidas para formar el **costo**, que incluye todos los ejemplos.\n\nRecuerda que para la regresión logística, la función de costo es de la forma:\n\n$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n\ndonde\n* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ es el costo para un solo punto de datos, que es:\n\n    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n    \n*  donde m es el número de ejemplos de entrenamiento en el conjunto de datos y:\n$$\n\\begin{align}\n  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n\\end{align}\n$$\n ",
      "metadata": {}
    },
    {
      "id": "93633a1e-6328-45d9-9998-4f46ea349077",
      "cell_type": "code",
      "source": "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\ny_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\nplt_simple_example(x_train, y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eab3d50c-c97d-4e76-b73a-8019603d5f41",
      "cell_type": "markdown",
      "source": "<a name='ex-02'></a>\n#### Descripción del código\n\nEl algoritmo para `compute_cost_logistic` recorre todos los ejemplos calculando la pérdida para cada uno y acumulando el total.\n\nNota que las variables X e y no son valores escalares sino matrices de forma(shape) ($m, n$) y ($m$,) respectivamente, donde $n$ es el número de X características y $m$ es el número de ejemplos de entrenamiento.\n",
      "metadata": {}
    },
    {
      "id": "73b9660d-6049-4767-8c45-ca7c2cdcec75",
      "cell_type": "code",
      "source": "plt.close('all')\nplt_logistic_squared_error(x_train,y_train)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff87c6c7-dc4a-4ff4-82c1-c87eeff095f7",
      "cell_type": "markdown",
      "source": "Verifica la implementación de la función de costo usando la siguiente celda.",
      "metadata": {}
    },
    {
      "id": "eb345577-2342-4970-9966-b3634b898fc4",
      "cell_type": "markdown",
      "source": "**Salida esperada**: 0.3668667864055175",
      "metadata": {}
    },
    {
      "id": "fac16b1c-9e35-4561-8ffa-bdd61ba836f6",
      "cell_type": "markdown",
      "source": "## Ejemplo\nAhora, veamos cuál es la salida de la función de costo para un valor diferente de $w$. \n\n* En un ejercicio anterior, graficaste la frontera de decisión para  $b = -3, w_0 = 1, w_1 = 1$. Es decir, tenías `b = -3, w = np.array([1,1])`.\n\n* Supón que quieres ver si $b = -4, w_0 = 1, w_1 = 1$, o `b = -4, w = np.array([1,1])` proporciona un mejor modelo.\n\nPrimero graficaremos la frontera de decisión para estos dos valores diferentes de $b$ para ver cuál se ajusta mejor a los datos.\n\n* Para $b = -3, w_0 = 1, w_1 = 1$, graficaremos $-3 + x_0+x_1 = 0$ (en azul)\n* Para $b = -4, w_0 = 1, w_1 = 1$, graficaremos $-4 + x_0+x_1 = 0$ (en magenta)",
      "metadata": {}
    },
    {
      "id": "ef4cdb70-7e76-4e40-96c0-f3aee2567a15",
      "cell_type": "code",
      "source": "plt_two_logistic_loss_curves()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7b025e2e-80c3-4767-987f-e7505ed96388",
      "cell_type": "markdown",
      "source": "Puedes ver en este gráfico que `b = -4, w = np.array([1,1])` es un peor modelo para los datos de entrenamiento. Veamos si la implementación de la función de costo refleja esto.",
      "metadata": {}
    },
    {
      "id": "77195e9f-8f59-4479-850f-457baf610d39",
      "cell_type": "markdown",
      "source": "The loss function above can be rewritten to be easier to implement.\n    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n  \nThis is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \nwhen $ y^{(i)} = 0$, the left-hand term is eliminated:\n$$\n\\begin{align}\nloss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n\\end{align}\n$$\nand when $ y^{(i)} = 1$, the right-hand term is eliminated:\n$$\n\\begin{align}\n  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n\\end{align}\n$$\n\nOK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:",
      "metadata": {}
    },
    {
      "id": "1bdf2094-0ba3-465f-8330-f01f6c2d9ac6",
      "cell_type": "code",
      "source": "plt.close('all')\ncst = plt_logistic_cost(x_train,y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d2cbf1ad-a1b0-4cda-b98b-840516e0a4ca",
      "cell_type": "markdown",
      "source": "**Salida esperada**\n\nCosto para b = -3 :  0.3668667864055175\n\nCosto para b = -4 :  0.5036808636748461\n\n\nPuedes ver que la función de costo se comporta como se espera y el costo para `b = -4, w = np.array([1,1])` es efectivamente mayor que el costo para `b = -3, w = np.array([1,1])`",
      "metadata": {}
    },
    {
      "id": "1ff3fdb7-aa3e-4d7d-9ba9-3d6bd5aa89b2",
      "cell_type": "markdown",
      "source": "## ¡Felicitaciones!\nEn este ejercicio examinaste y utilizaste la función de costo para regresión logística.",
      "metadata": {}
    }
  ]
}