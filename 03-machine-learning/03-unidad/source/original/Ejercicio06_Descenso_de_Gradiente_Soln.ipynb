{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "09d6030c-32dc-43e3-b1de-c95413167adf",
      "cell_type": "markdown",
      "source": "# Ejercicio: Descenso de Gradiente para Regresión Logística",
      "metadata": {}
    },
    {
      "id": "13fdb251-c1e9-46fb-b7f7-d999a46eb4b0",
      "cell_type": "markdown",
      "source": "## Objetivos\nEn este ejercicio, vas a:\n- actualizar el descenso de gradiente para regresión logística.\n- explorar el descenso de gradiente en un conjunto de datos conocido",
      "metadata": {}
    },
    {
      "id": "cd7dcf2f-cabd-4d1a-bd8a-a771ed7b862c",
      "cell_type": "code",
      "source": "import copy, math\nimport numpy as np\n#%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\nfrom plt_quad_logistic import plt_quad_logistic, plt_prob\nplt.style.use('./deeplearning.mplstyle')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fdda4d51-a3e4-4b59-87ea-d0703cc4cca8",
      "cell_type": "markdown",
      "source": "## Conjunto de datos\nComencemos con el mismo conjunto de datos de dos X características usado en el ejercicio de frontera de decisión.",
      "metadata": {}
    },
    {
      "id": "adf11dd0-7f80-4f73-818a-266091e0cf44",
      "cell_type": "code",
      "source": "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_train = np.array([0, 0, 0, 1, 1, 1])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bff9c316-f214-4da3-8161-ffe02a80cccf",
      "cell_type": "markdown",
      "source": "Como antes, usaremos una función auxiliar para graficar estos datos. Los puntos de datos con etiqueta $y=1$ se muestran como cruces rojas, mientras que los puntos con $y=0$ se muestran como círculos azules.",
      "metadata": {}
    },
    {
      "id": "212bb827-f2f1-4019-9c64-fec561574680",
      "cell_type": "code",
      "source": "fig,ax = plt.subplots(1,1,figsize=(4,4))\nplot_data(X_train, y_train, ax)\n\nax.axis([0, 4, 0, 3.5])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d7d01e23-b3be-4b93-b605-29200d4d9d7d",
      "cell_type": "markdown",
      "source": "## Descenso de Gradiente Logístico\n<img align=\"right\" src=\"./images/C1_W3_Logistic_gradient_descent.png\"     style=\" width:400px; padding: 10px; \" >\n\nRecuerda que el algoritmo de descenso de gradiente utiliza el cálculo del gradiente:\n$$\\begin{align*}\n&\\text{repetir hasta convergencia:} \\; \\lbrace \\\\\n&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{para j := 0..n-1} \\\\ \n&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n&\\rbrace\n\\end{align*}$$\n\nDonde cada iteración realiza actualizaciones simultáneas en $w_j$ para todos los $j$, donde\n$$\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n\\end{align*}$$\n\n* m es el número de ejemplos de entrenamiento en el conjunto de datos      \n* $f_{\\mathbf{w},b}(x^{(i)})$ es la predicción del modelo, mientras que $y^{(i)}$ es el objetivo\n* Para un modelo de regresión logística  \n    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n    $f_{\\mathbf{w},b}(x) = g(z)$  \n    donde $g(z)$ es la función sigmoidea:  \n    $g(z) = \\frac{1}{1+e^{-z}}$   \n    \n",
      "metadata": {}
    },
    {
      "id": "c3d1681c-e310-468b-8499-42cfaf4cab48",
      "cell_type": "markdown",
      "source": "### Implementación del Descenso de Gradiente\nLa implementación del algoritmo de descenso de gradiente tiene dos componentes: \n- El ciclo que implementa la ecuación (1) anterior. Esto es `gradient_descent` abajo y generalmente se proporciona en ejercicios opcionales y de práctica.\n- El cálculo del gradiente actual, ecuaciones (2,3) anteriores. Esto es `compute_gradient_logistic` abajo. Se te pedirá que implementes este ejercicio de la semana.\n\n#### Cálculo del Gradiente, Descripción del Código\nImplementa las ecuaciones (2),(3) anteriores para todos los $w_j$ y $b$.\nHay muchas formas de implementarlo. A continuación se describe esta:\n- inicializar variables para acumular `dj_dw` y `dj_db`\n- para cada ejemplo\n    - calcular el error para ese ejemplo $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$\n    - para cada valor de entrada $x_{j}^{(i)}$ en este ejemplo,  \n        - multiplicar el error por la entrada  $x_{j}^{(i)}$, y sumar al elemento correspondiente de `dj_dw`. (ecuación 2 anterior)\n    - sumar el error a `dj_db` (ecuación 3 anterior)\n\n- dividir `dj_db` y `dj_dw` por el número total de ejemplos (m)\n- nota que $\\mathbf{x}^{(i)}$ en numpy es `X[i,:]` o `X[i]`  y $x_{j}^{(i)}$ es `X[i,j]`",
      "metadata": {}
    },
    {
      "id": "2ee8c5d8-190b-4f35-987f-2e196da6ce29",
      "cell_type": "code",
      "source": "def compute_gradient_logistic(X, y, w, b): \n    \"\"\"\n    Calcula el gradiente para regresión logística \n \n    Args:\n      X (ndarray (m,n): Datos, m ejemplos con n características\n      y (ndarray (m,)): valores obejtivo\n      w (ndarray (n,)): parametros del modelo\n      b (scalar)      : parametro del modelo\n    Returns\n      dj_dw (ndarray (n,)): El gradiente del costo w.r.t. los parametros de w. \n      dj_db (scalar)      : El gradiente del costo w.r.t. el parametro b. \n    \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros((n,))                           #(n,)\n    dj_db = 0.\n\n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n        err_i  = f_wb_i  - y[i]                       #scalar\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   #(n,)\n    dj_db = dj_db/m                                   #scalar\n        \n    return dj_db, dj_dw  ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "40eaf923-8666-4ea2-aae1-f1022b08fd11",
      "cell_type": "markdown",
      "source": "Verifica la implementación de la función de gradiente usando la siguiente celda.",
      "metadata": {}
    },
    {
      "id": "2e2c5408-73f1-4591-8787-371cd3d1715b",
      "cell_type": "code",
      "source": "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_tmp = np.array([0, 0, 0, 1, 1, 1])\nw_tmp = np.array([2.,3.])\nb_tmp = 1.\ndj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\nprint(f\"dj_db: {dj_db_tmp}\" )\nprint(f\"dj_dw: {dj_dw_tmp.tolist()}\" )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e691749e-e7f5-477c-929f-2d1b68f72843",
      "cell_type": "markdown",
      "source": "**Salida esperada**\n``` \ndj_db: 0.49861806546328574\ndj_dw: [0.498333393278696, 0.49883942983996693]\n```",
      "metadata": {}
    },
    {
      "id": "55e0a371-30e9-47ca-80c7-1f473dc99bf7",
      "cell_type": "markdown",
      "source": "#### Código de Descenso de Gradiente \nEl código que implementa la ecuación (1) anterior se muestra a continuación. Tómate un momento para ubicar y comparar las funciones en la rutina con las ecuaciones anteriores.",
      "metadata": {}
    },
    {
      "id": "a94cce6d-ed16-445b-a511-4b3cb7e04cc8",
      "cell_type": "code",
      "source": "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent\n    \n    Args:\n      X (ndarray (m,n)   : Datos, m ejemplos con n características\n      y (ndarray (m,))   : valores objetivo\n      w (ndarray (n,))   : Valores iniciales de los parametros del modelo\n      b (scalar)         : Valores iniciales de los parametro del modelo\n      alpha (float)      : Tasa de aprendizaje\n      num_iters (scalar) : número de iteraciones para correr el descenso gradiente\n      \n    Returns:\n      w (ndarray (n,))   : Valores de parametros actualizados Updated values of parameters\n      b (scalar)         : Valor de parametro actualizado \n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_iters):\n        # Calculate the gradient and update the parameters\n        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n      \n        # Save cost J at each iteration\n        if i<100000:      # prevent resource exhaustion \n            J_history.append( compute_cost_logistic(X, y, w, b) )\n\n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteracion {i:4d}: Costo {J_history[-1]}   \")\n        \n    return w, b, J_history         #return final w,b and J history for graphing\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f5a11d55-e65f-4a2b-84f7-fe0722e5b9ae",
      "cell_type": "markdown",
      "source": "Ejecutemos el descenso de gradiente en nuestro conjunto de datos.",
      "metadata": {}
    },
    {
      "id": "5011ca8f-b57b-4b96-97fd-0b5592f569b0",
      "cell_type": "code",
      "source": "w_tmp  = np.zeros_like(X_train[0])\nb_tmp  = 0.\nalph = 0.1\niters = 10000\n\nw_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \nprint(f\"\\nparametros actualizados: w:{w_out}, b:{b_out}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9b3d3d07-a957-4377-9f8e-2816100fd487",
      "cell_type": "markdown",
      "source": "#### Grafiquemos los resultados del descenso de gradiente:",
      "metadata": {}
    },
    {
      "id": "290b0ae5-43de-4c24-bea2-20daeb5b4f45",
      "cell_type": "code",
      "source": "fig,ax = plt.subplots(1,1,figsize=(5,4))\n# plot the probability \nplt_prob(ax, w_out, b_out)\n\n# Plot the original data\nax.set_ylabel(r'$x_1$')\nax.set_xlabel(r'$x_0$')   \nax.axis([0, 4, 0, 3.5])\nplot_data(X_train,y_train,ax)\n\n# Plot the decision boundary\nx0 = -b_out/w_out[0]\nx1 = -b_out/w_out[1]\nax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ae030356-2b6e-459c-9e33-0b7789e1bf0b",
      "cell_type": "markdown",
      "source": "En el gráfico anterior:\n - el sombreado refleja la probabilidad y=1 (resultado antes de la frontera de decisión)\n - la frontera de decisión es la línea donde la probabilidad = 0.5\n ",
      "metadata": {}
    },
    {
      "id": "165c86b2-46ce-43ee-8be0-29045dda0ae8",
      "cell_type": "markdown",
      "source": "## Otro conjunto de datos\nVolvamos a un conjunto de datos de una sola variable. Con solo dos parámetros, $w$, $b$, es posible graficar la función de costo usando un gráfico de contorno para tener una mejor idea de lo que hace el descenso de gradiente.",
      "metadata": {}
    },
    {
      "id": "1dd79376-a569-4b39-9c5b-af4a6783e7ff",
      "cell_type": "code",
      "source": "x_train = np.array([0., 1, 2, 3, 4, 5])\ny_train = np.array([0,  0, 0, 1, 1, 1])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9baefa72-3e0a-4cd8-8214-38811aeaedad",
      "cell_type": "markdown",
      "source": "Como antes, usaremos una función auxiliar para graficar estos datos. Los puntos de datos con etiqueta $y=1$ se muestran como cruces rojas, mientras que los puntos con $y=0$ se muestran como círculos azules.",
      "metadata": {}
    },
    {
      "id": "e7af1a65-9fd6-401d-8617-ef80cdcbc323",
      "cell_type": "code",
      "source": "fig,ax = plt.subplots(1,1,figsize=(4,3))\nplt_tumor_data(x_train, y_train, ax)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dccbf5c7-7a7d-44d1-9bb8-942b4a2b9d31",
      "cell_type": "markdown",
      "source": "En el gráfico a continuación, prueba:\n- cambiar $w$ y $b$ haciendo clic dentro del gráfico de contorno en la parte superior derecha.\n    - los cambios pueden tardar uno o dos segundos\n    - observa el valor cambiante del costo en el gráfico superior izquierdo.\n    - observa que el costo se acumula por una pérdida en cada ejemplo (líneas punteadas verticales)\n- ejecuta el descenso de gradiente haciendo clic en el botón naranja.\n    - observa el costo disminuyendo constantemente (el gráfico de contorno y el de costo están en log(costo))\n    - hacer clic en el gráfico de contorno reiniciará el modelo para una nueva ejecución\n- para reiniciar el gráfico, vuelve a ejecutar la celda",
      "metadata": {}
    },
    {
      "id": "a98f2c95-4452-40dc-8718-1fc356541589",
      "cell_type": "code",
      "source": "w_range = np.array([-1, 7])\nb_range = np.array([1, -14])\nquad = plt_quad_logistic( x_train, y_train, w_range, b_range )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "25be7f07-df0a-42c7-85e3-0ea42da24024",
      "cell_type": "markdown",
      "source": "## ¡Felicitaciones!\nHas:\n- examinado las fórmulas e implementación del cálculo del gradiente para regresión logística\n- utilizado esas rutinas en\n    - exploración de un conjunto de datos de una sola variable\n    - exploración de un conjunto de datos de dos variables",
      "metadata": {}
    },
    {
      "id": "828aab0d-edc9-4c95-b9fe-a8dec7f11557",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}