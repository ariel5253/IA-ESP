{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    },
    "toc-autonumbering": false
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "acde3031-45f2-40c8-935a-098c0c37a551",
      "cell_type": "markdown",
      "source": "# Ejercicio: Escalado de características y tasa de aprendizaje (Multi-variable)",
      "metadata": {}
    },
    {
      "id": "0302e98f-5542-4a3d-b821-0b216c278083",
      "cell_type": "markdown",
      "source": "## Objetivos\nEn este ejercicio usted:\n- Utilizará las rutinas de múltiples variables desarrolladas en el ejercicio anterior\n- Ejecutará descenso de gradiente en un conjunto de datos con múltiples características\n- Explorará el impacto de la *tasa de aprendizaje alpha* en el descenso de gradiente\n- Mejorará el rendimiento del descenso de gradiente mediante *escalado de características* usando normalización z-score",
      "metadata": {}
    },
    {
      "id": "7b1a4ddf-30f7-46d1-bbed-26e3e46061af",
      "cell_type": "markdown",
      "source": "## Herramientas\nUtilizará las funciones desarrolladas en el ejercicio anterior así como matplotlib y NumPy. ",
      "metadata": {}
    },
    {
      "id": "6e381759-dcb5-497f-bc89-75c590d7d6c6",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom ejerc_utils_multi import  load_house_data, run_gradient_descent \nfrom ejerc_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w\nfrom ejerc_utils_common import dlc\nnp.set_printoptions(precision=2)\nplt.style.use('./deeplearning.mplstyle')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a7feaf6-98f1-448e-9ee3-55621f84c207",
      "cell_type": "markdown",
      "source": "## Notación\n\n| Notación General | Descripción | Python (si aplica) |\n|:----------------|:------------------------------------------------------------|:---------------------|\n| $a$ | escalar, no en negrita | |\n| $\\mathbf{a}$ | vector, en negrita | |\n| $\\mathbf{A}$ | matriz, mayúscula en negrita | |\n| **Regresión** | | |\n| $\\mathbf{X}$ | matriz de ejemplos de entrenamiento | `X_train` |\n| $\\mathbf{y}$ | objetivos de ejemplos de entrenamiento | `y_train` |\n| $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i$-ésimo ejemplo de entrenamiento | `X[i]`, `y[i]` |\n| $m$ | número de ejemplos de entrenamiento | `m` |\n| $n$ | número de características en cada ejemplo | `n` |\n| $\\mathbf{w}$ | parámetro: peso | `w` |\n| $b$ | parámetro: sesgo | `b` |\n| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | Resultado de la evaluación del modelo en $\\mathbf{x}^{(i)}$ parametrizado por $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$ | `f_wb` |\n|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$| el gradiente o derivada parcial del costo respecto a un parámetro $w_j$ |`dj_dw[j]`| \n|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$| el gradiente o derivada parcial del costo respecto a un parámetro $b$| `dj_db`|",
      "metadata": {}
    },
    {
      "id": "16edb239-227c-4911-bd19-1a6646381b44",
      "cell_type": "markdown",
      "source": "# Enunciado del Problema\n\nComo en los ejercicios anteriores, usará el ejemplo motivador de predicción de precios de viviendas. El conjunto de datos de entrenamiento contiene muchos ejemplos con 4 características (tamaño, habitaciones, pisos y antigüedad) mostrados en la tabla a continuación. Nota: en este ejercicio, la característica Tamaño está en metros² mientras que en ejercicios anteriores se utilizó miles de metros². Este conjunto de datos es más grande que el del ejercicio anterior.\n\nQueremos construir un modelo de regresión lineal usando estos valores para luego predecir el precio de otras casas, por ejemplo, una casa de 1200 metros², 3 habitaciones, 1 piso, 40 años de antigüedad.\n\n## Conjunto de datos: \n| Tamaño (metros²) | Número de Habitaciones | Número de Pisos | Antigüedad de la Casa | Precio (miles de dólares) |\n|----------------|----------------------|-----------------|----------------------|---------------------------|\n| 952            | 2                    | 1               | 65                   | 271.5                     |\n| 1244           | 3                    | 2               | 64                   | 232                       |\n| 1947           | 3                    | 2               | 17                   | 509.8                     |\n| ...            | ...                  | ...             | ...                  | ...                       |\n",
      "metadata": {}
    },
    {
      "id": "342881f9-399f-43ba-929a-4478422fb840",
      "cell_type": "code",
      "source": "# cargar el conjunto de datos\nX_train, y_train = load_house_data()\nX_features = ['tamaño(metros²)','bedrooms','floors','age']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ee6f6ef3-0b58-4749-8a8a-14991e31bc4b",
      "cell_type": "markdown",
      "source": "Visualicemos el conjunto de datos y sus características graficando cada característica versus el precio.",
      "metadata": {}
    },
    {
      "id": "16329e02-ffb2-4924-87aa-193952756a0b",
      "cell_type": "code",
      "source": "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(X_train[:,i],y_train)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"Precio (miles)\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4954ed54-3b1f-4f6d-bfe4-52dff1cf9ba9",
      "cell_type": "markdown",
      "source": "Graficar cada característica vs. el objetivo, precio, da una idea de qué características tienen mayor influencia en el precio. Arriba, aumentar el tamaño también aumenta el precio. Las habitaciones y los pisos no parecen tener un gran impacto en el precio. Las casas más nuevas tienen precios más altos que las casas más antiguas.",
      "metadata": {}
    },
    {
      "id": "ab10d1b9-4491-4917-8b99-bb2ea11e13e2",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_5\"></a>\n## Descenso de Gradiente con Múltiples Variables\nAquí están las ecuaciones que desarrolló en el ejercicio anterior sobre descenso de gradiente para múltiples variables.:\n\n$$\\begin{align*} \\text{repetir}&\\text{ hasta convergencia:} \\; \\lbrace \\newline\\;\n& w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{para j = 0..n-1}\\newline\n&b\\ \\ := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}$$\n\ndonde, n es el número de características, los parámetros $w_j$,  $b$, se actualizan simultáneamente y donde  \n\n$$\n\\begin{align}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n\\end{align}\n$$\n* m es el número de ejemplos de entrenamiento en el conjunto de datos\n\n    \n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ es la predicción del modelo, mientras que $y^{(i)}$ es el valor objetivo\n",
      "metadata": {}
    },
    {
      "id": "3f0c53fb-7eb0-4f2a-a800-213a507b58ba",
      "cell_type": "markdown",
      "source": "## Tasa de Aprendizaje\n\nLas lecciones discutieron algunos de los problemas relacionados con el ajuste de la tasa de aprendizaje $\\alpha$. La tasa de aprendizaje controla el tamaño de la actualización de los parámetros. Ver la ecuación (1) anterior. Es compartida por todos los parámetros.\n\nEjecutemos descenso de gradiente y probemos algunos valores de $\\alpha$ en nuestro conjunto de datos.",
      "metadata": {}
    },
    {
      "id": "9a00b9ac-421e-4e6c-a7b6-62feb07cbc05",
      "cell_type": "markdown",
      "source": "### $\\alpha$ = 9.9e-7",
      "metadata": {}
    },
    {
      "id": "6004674e-58dd-450c-b101-39242d2d2bf2",
      "cell_type": "code",
      "source": "# establecer alpha en 9.9e-7\n_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9981bb3f-f501-4289-a455-fea951ecfe30",
      "cell_type": "markdown",
      "source": "Parece que la tasa de aprendizaje es demasiado alta. La solución no converge. El costo está *aumentando* en lugar de disminuir. Grafiquemos el resultado:",
      "metadata": {}
    },
    {
      "id": "141bb35f-9924-48d2-a62a-4f4d36013f4a",
      "cell_type": "code",
      "source": "plot_cost_i_w(X_train, y_train, hist)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a90a5ff8-b5a9-4de3-b1a5-f21931dd7610",
      "cell_type": "markdown",
      "source": "El gráfico de la derecha muestra el valor de uno de los parámetros, $w_0$. En cada iteración, sobrepasa el valor óptimo y como resultado, el costo termina *aumentando* en lugar de acercarse al mínimo. Tenga en cuenta que esto no es una imagen completamente precisa ya que hay 4 parámetros que se modifican en cada pasada y no solo uno. Este gráfico solo muestra $w_0$ con los otros parámetros fijos en valores benignos. En este y en gráficos posteriores puede notar que las líneas azul y naranja están ligeramente desfasadas.",
      "metadata": {}
    },
    {
      "id": "45f7a3a8-0c90-4a62-9618-e9790bad9dc2",
      "cell_type": "markdown",
      "source": "\n### $\\alpha$ = 9e-7\nProbemos un valor un poco menor y veamos qué sucede.",
      "metadata": {}
    },
    {
      "id": "6b0ed91c-88a6-4625-949d-49a605c3377c",
      "cell_type": "code",
      "source": "# establecer alpha en 9e-7\n_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a08d2609-1180-4708-a5d1-387689b83e47",
      "cell_type": "markdown",
      "source": "El costo disminuye durante toda la ejecución mostrando que alpha no es demasiado grande. ",
      "metadata": {}
    },
    {
      "id": "75b66890-8007-4eeb-9bab-182ed057ce00",
      "cell_type": "code",
      "source": "plot_cost_i_w(X_train, y_train, hist)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b3486cf7-f67f-4d8c-85e2-5c12584bde11",
      "cell_type": "markdown",
      "source": "A la izquierda, se observa que el costo disminuye como debe ser. A la derecha, se puede ver que $w_0$ todavía oscila alrededor del mínimo, pero el costo disminuye con cada iteración en lugar de aumentar. Note arriba que `dj_dw[0]` cambia de signo en cada iteración a medida que `w[0]` salta sobre el valor óptimo.\nEste valor de alpha convergerá. Puede variar el número de iteraciones para ver cómo se comporta.",
      "metadata": {}
    },
    {
      "id": "5c1897d5-3f52-4c8f-b126-388f65cdb8fb",
      "cell_type": "markdown",
      "source": "### $\\alpha$ = 1e-7\nProbemos un valor aún más pequeño para $\\alpha$ y veamos qué sucede.",
      "metadata": {}
    },
    {
      "id": "9737b2e5-8f9a-42e6-85f8-8c5d13bfd15b",
      "cell_type": "code",
      "source": "# establecer alpha en 1e-7\n_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "be8e1dd0-0864-4f58-a6b8-a74c79e0fbf4",
      "cell_type": "markdown",
      "source": "El costo disminuye durante toda la ejecución mostrando que $\\alpha$ no es demasiado grande. ",
      "metadata": {}
    },
    {
      "id": "8648f940-f305-4518-903c-7841b0154af0",
      "cell_type": "code",
      "source": "plot_cost_i_w(X_train,y_train,hist)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "060a5fad-0b37-4890-8c52-19eda98bb291",
      "cell_type": "markdown",
      "source": "A la izquierda, se observa que el costo disminuye como debe ser. A la derecha, se puede ver que $w_0$ se acerca al mínimo sin oscilaciones. `dj_w0` es negativo durante toda la ejecución. Esta solución también convergerá.",
      "metadata": {}
    },
    {
      "id": "ef4a84b4-7abb-424c-82ad-d1cc78831782",
      "cell_type": "markdown",
      "source": "## Escalado de Características (escalado de x/entradas)\n\nLas lecciones describieron la importancia de reescalar el conjunto de datos para que las características tengan un rango similar.\nSi le interesa el detalle de por qué esto es importante, haga clic en el encabezado 'detalles' a continuación. Si no, la siguiente sección mostrará cómo implementar el escalado de características.",
      "metadata": {
        "tags": []
      }
    },
    {
      "id": "3ed695cf-61af-48b1-bd2e-61cba543d01d",
      "cell_type": "markdown",
      "source": "<details>\n<summary>\n    <font size='3', color='darkgreen'><b>Detalles</b></font>\n</summary>\n\nVeamos nuevamente la situación con $\\alpha$ = 9e-7. Este valor está bastante cerca del máximo que podemos establecer para $\\alpha$ sin que diverja. Esta es una ejecución corta mostrando las primeras iteraciones:\n\n<figure>\n    <img src=\"./images/C1_W2_Lab06_ShortRun.PNG\" style=\"width:1200px;\" >\n</figure>\n\nArriba, aunque el costo está disminuyendo, es claro que $w_0$ progresa mucho más rápido que los otros parámetros debido a su gradiente mucho mayor.\n\nEl gráfico siguiente muestra el resultado de una ejecución muy larga con $\\alpha$ = 9e-7. Esto toma varias horas.\n\n<figure>\n    <img src=\"./images/C1_W2_Lab06_LongRun.PNG\" style=\"width:1200px;\" >\n</figure>\n    \nArriba, puede ver que el costo disminuyó lentamente después de la reducción inicial. Note la diferencia entre `w0` y `w1`,`w2`,`w3` así como entre `dj_dw0` y `dj_dw1-3`. `w0` alcanza su valor casi final muy rápido y `dj_dw0` disminuye rápidamente a un valor pequeño mostrando que `w0` está cerca del valor final. Los otros parámetros se redujeron mucho más lentamente.\n\n¿Por qué ocurre esto? ¿Hay algo que podamos mejorar? Vea abajo:\n<figure>\n    <center> <img src=\"./images/C1_W2_Lab06_scale.PNG\"   ></center>\n</figure>   \n\nLa figura anterior muestra por qué los $w$ se actualizan de manera desigual. \n- $\\alpha$ es compartido por todas las actualizaciones de parámetros ($w$ y $b$).\n- el término de error común se multiplica por las características para los $w$ (no para $b$).\n- las características varían significativamente en magnitud haciendo que algunas se actualicen mucho más rápido que otras. En este caso, $w_0$ se multiplica por 'size(sqft)', que generalmente es > 1000, mientras que $w_1$ se multiplica por 'number of bedrooms', que generalmente es 2-4. \n    \nLa solución es el escalado de características.",
      "metadata": {}
    },
    {
      "id": "7858e66b-09ef-483a-aa11-208d8d871053",
      "cell_type": "markdown",
      "source": "Las lecciones discutieron tres técnicas diferentes: \n- Escalado de características, que esencialmente divide cada característica positiva por su valor máximo, o más generalmente, reescala cada característica por su mínimo y máximo usando (x-min)/(max-min). Ambas formas normalizan las características al rango de -1 a 1, donde el primer método funciona para características positivas y es simple, y el segundo funciona para cualquier característica.\n- Normalización por la media: $x_i := \\dfrac{x_i - \\mu_i}{max - min} $ \n- Normalización z-score, que exploraremos a continuación. ",
      "metadata": {}
    },
    {
      "id": "c39c8e43-da83-44cd-a361-33c9a7a42bcc",
      "cell_type": "markdown",
      "source": "\n### Normalización z-score \nDespués de la normalización z-score, todas las características tendrán media 0 y desviación estándar 1.\n\nPara implementar la normalización z-score, ajuste sus valores de entrada como se muestra en esta fórmula:\n$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$ \ndonde $j$ selecciona una característica o columna en la matriz $\\mathbf{X}$. $µ_j$ es la media de todos los valores para la característica (j) y $\\sigma_j$ es la desviación estándar de la característica (j).\n$$\n\\begin{align}\n\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n\\end{align}\n$$\n\n**Nota de implementación:** Al normalizar las características, es importante guardar los valores usados para la normalización: la media y la desviación estándar utilizadas en los cálculos. Después de aprender los parámetros del modelo, a menudo queremos predecir precios de casas que no hemos visto antes. Dado un nuevo valor x (área de la sala y número de habitaciones), primero debemos normalizar x usando la media y desviación estándar previamente calculadas del conjunto de entrenamiento.\n\n**Implementación**",
      "metadata": {}
    },
    {
      "id": "8f4cd15b-8704-4b25-af14-7a13f243cfe3",
      "cell_type": "code",
      "source": "def zscore_normalize_features(X):\n    \"\"\"\n    calcula X, normalizado z-score por columna\n    \n    Argumentos:\n      X (ndarray (m,n))     : datos de entrada, m ejemplos, n características\n      \n    Retorna:\n      X_norm (ndarray (m,n)): entrada normalizada por columna\n      mu (ndarray (n,))     : media de cada característica\n      sigma (ndarray (n,))  : desviación estándar de cada característica\n    \"\"\"\n    # encontrar la media de cada columna/característica\n    mu     = np.mean(X, axis=0)                 # mu tendrá forma/shape (n,)\n    # encontrar la desviación estándar de cada columna/característica\n    sigma  = np.std(X, axis=0)                  # sigma tendrá forma/shape (n,)\n    # restar la media y dividir por la desviación estándar para cada columna\n    X_norm = (X - mu) / sigma      \n\n    return (X_norm, mu, sigma)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "91a85d84-5456-4b15-9125-6a1fac35fca1",
      "cell_type": "markdown",
      "source": "Veamos los pasos involucrados en la normalización z-score. El gráfico a continuación muestra la transformación paso a paso.",
      "metadata": {}
    },
    {
      "id": "1fcbeab8-144e-4630-b854-3ab3732f2ce8",
      "cell_type": "code",
      "source": "mu     = np.mean(X_train,axis=0)   \nsigma  = np.std(X_train,axis=0) \nX_mean = (X_train - mu)\nX_norm = (X_train - mu)/sigma      \n\nfig,ax=plt.subplots(1, 3, figsize=(12, 3))\nax[0].scatter(X_train[:,0], X_train[:,3])\nax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\nax[0].set_title(\"no-normalizado\")\nax[0].axis('equal')\n\nax[1].scatter(X_mean[:,0], X_mean[:,3])\nax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\nax[1].set_title(r\"X - $\\mu$\")\nax[1].axis('equal')\n\nax[2].scatter(X_norm[:,0], X_norm[:,3])\nax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\nax[2].set_title(r\"Z-score normalizado\")\nax[2].axis('equal')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nfig.suptitle(\"distribución de entradas (características) antes, durante, después de la normalización\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a52b48c6-48de-4170-90f3-f4c9b3fca960",
      "cell_type": "markdown",
      "source": "El gráfico anterior muestra la relación entre dos de los parámetros del conjunto de entrenamiento, \"antigüedad\" y \"tamaño (pies²)\". *Estos se grafican con escala igual*. \n- Izquierda: Sin normalizar: El rango de valores o la varianza de la característica 'tamaño (pies²)' es mucho mayor que la de antigüedad.\n- Centro: El primer paso elimina la media o valor promedio de cada característica. Esto deja las características centradas en cero. Es difícil ver la diferencia para la característica 'antigüedad', pero 'tamaño (pies²)' está claramente alrededor de cero.\n- Derecha: El segundo paso divide por la desviación estándar. Esto deja ambas características centradas en cero y con una escala similar.",
      "metadata": {}
    },
    {
      "id": "766b1560-23bc-4966-b884-ffb6a89d8f54",
      "cell_type": "markdown",
      "source": "Normalicemos los datos y compáremoslos con los datos originales.",
      "metadata": {}
    },
    {
      "id": "718eeadc-d4b7-46a4-b48e-a888e72b7a26",
      "cell_type": "code",
      "source": "# normalizar las entradas (características) originales\nX_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\nprint(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\nprint(f\"Rango pico a pico por columna en X original: {np.ptp(X_train,axis=0)}\")   \nprint(f\"Rango pico a pico por columna en X normalizado: {np.ptp(X_norm,axis=0)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d66b14bd-916e-46eb-aebe-7a6bb9e1e3bc",
      "cell_type": "markdown",
      "source": "El rango pico a pico de cada columna se reduce de un factor de miles a un factor de 2-3 mediante la normalización.",
      "metadata": {}
    },
    {
      "id": "ceb6ea13-b2a3-4759-8600-75bfea3c8c6f",
      "cell_type": "code",
      "source": "fig,ax=plt.subplots(1, 4, figsize=(12, 3))\nfor i in range(len(ax)):\n    norm_plot(ax[i],X_train[:,i],)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"conteo\");\nfig.suptitle(\"distribución de entradas (características) antes de la normalización\")\nplt.show()\nfig,ax=plt.subplots(1,4,figsize=(12,3))\nfor i in range(len(ax)):\n    norm_plot(ax[i],X_norm[:,i],)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"conteo\"); \nfig.suptitle(\"distribución de entradas (características) después de la normalización\")\n\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7c4ff264-f599-4ad6-839e-328741249fc1",
      "cell_type": "markdown",
      "source": "Observe arriba que el rango de los datos normalizados (eje x) está centrado alrededor de cero y aproximadamente entre +/- 2. Lo más importante es que el rango es similar para cada característica.",
      "metadata": {}
    },
    {
      "id": "cd69b251-ed20-4e4b-8d23-f88eb207349f",
      "cell_type": "markdown",
      "source": "Volvamos a ejecutar nuestro algoritmo de descenso de gradiente con los datos normalizados.\nObserve el **valor mucho mayor de alpha**. Esto acelerará el descenso de gradiente.",
      "metadata": {}
    },
    {
      "id": "912af6ff-1985-47f7-99dd-47c39b2a5e12",
      "cell_type": "code",
      "source": "w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6027e2fb-89b5-490c-bfc9-e503424ce66a",
      "cell_type": "markdown",
      "source": "¡Las características escaladas logran resultados muy precisos **mucho, mucho más rápido**! Observe que el gradiente de cada parámetro es diminuto al final de esta ejecución relativamente corta. Una tasa de aprendizaje de 0.1 es un buen inicio para regresión con características normalizadas.\nGrafiquemos nuestras predicciones versus los valores objetivo. Nota: la predicción se hace usando la característica normalizada mientras que el gráfico se muestra usando los valores originales.",
      "metadata": {}
    },
    {
      "id": "484ff077-a3a8-48c3-9548-7bf653641933",
      "cell_type": "code",
      "source": "# Predice el objetivo usando las entradas (características) X normalizadas\nm = X_norm.shape[0]\nyp = np.zeros(m)\nfor i in range(m):\n    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n\n    # Grafica las predicciones y objetivos versus las entradas (características) X originales    \nfig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(X_train[:,i],y_train, label = 'objetivo')\n    ax[i].set_xlabel(X_features[i])\n    ax[i].scatter(X_train[:,i],yp,color=dlc[\"dlorange\"], label = 'predicción')\nax[0].set_ylabel(\"Precio\"); ax[0].legend();\nfig.suptitle(\"Objetivo versus predicción usando el modelo normalizado z-score\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2c128b1-37e8-44e3-b9a4-0ed46c018f4f",
      "cell_type": "markdown",
      "source": "Los resultados se ven bien. Algunos puntos a destacar:\n- con múltiples características, ya no podemos tener un solo gráfico mostrando resultados versus características.\n- al generar el gráfico, se usaron las características normalizadas. Cualquier predicción usando los parámetros aprendidos de un conjunto de entrenamiento normalizado también debe ser normalizada.",
      "metadata": {}
    },
    {
      "id": "1159833f-b121-43be-b7ce-6fff3d6da687",
      "cell_type": "markdown",
      "source": "**Predicción**\nEl objetivo de generar nuestro modelo es usarlo para predecir precios de viviendas que no están en el conjunto de datos. Predigamos el precio de una casa de 1200 pies², 3 habitaciones, 1 piso, 40 años de antigüedad. Recuerde que debe normalizar los datos con la media y desviación estándar obtenidas al normalizar los datos de entrenamiento. ",
      "metadata": {}
    },
    {
      "id": "5265e662-75a8-4cf1-9ded-771e78d11adc",
      "cell_type": "code",
      "source": "# Primero, normalizar nuestro ejemplo.\nx_house = np.array([1200, 3, 1, 40])\nx_house_norm = (x_house - X_mu) / X_sigma\nprint(x_house_norm)\nx_house_predict = np.dot(x_house_norm, w_norm) + b_norm\nprint(f\" precio predicho de una casa de 1200 metros², 3 habitaciones, 1 piso, 40 años de antigüedad = ${x_house_predict*1000:0.0f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1ff2e52b-69e4-4f48-a2cd-d6b7010431c3",
      "cell_type": "markdown",
      "source": "**Contornos de Costo**  \nOtra forma de ver el escalado de características es en términos de los contornos de costo. Cuando las escalas de las características no coinciden, el gráfico de costo versus parámetros en un gráfico de contorno es asimétrico. \n\nEn el gráfico de abajo, la escala de los parámetros está igualada. El gráfico de la izquierda es el contorno de costo de w[0], los pies cuadrados versus w[1], el número de habitaciones antes de normalizar las características. El gráfico es tan asimétrico que las curvas que completan los contornos no son visibles. En contraste, cuando las características están normalizadas, el contorno de costo es mucho más simétrico. El resultado es que las actualizaciones de los parámetros durante el descenso de gradiente pueden avanzar igual para cada parámetro. \n",
      "metadata": {}
    },
    {
      "id": "0d3c23b5-df8c-48da-91c6-72ffc39b1f04",
      "cell_type": "code",
      "source": "plt_equal_scale(X_train, X_norm, y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7d828b83-6cff-4d1f-8258-def36050842f",
      "cell_type": "markdown",
      "source": "\n## ¡Felicitaciones!\nEn este ejercicio usted:\n- utilizó las rutinas para regresión lineal con múltiples características desarrolladas en ejercicios anteriores\n- exploró el impacto de la tasa de aprendizaje $\\alpha$ en la convergencia \n- descubrió el valor del escalado de características usando normalización z-score para acelerar la convergencia",
      "metadata": {}
    },
    {
      "id": "6e09ca98-b36a-484a-93ff-d47daa8444e1",
      "cell_type": "markdown",
      "source": "## Reconocimiento Legal\nLos datos de viviendas fueron derivados del [Ames Housing dataset](http://jse.amstat.org/v19n3/decock.pdf) compilado por Dean De Cock para su uso en educación en ciencia de datos.",
      "metadata": {}
    }
  ]
}