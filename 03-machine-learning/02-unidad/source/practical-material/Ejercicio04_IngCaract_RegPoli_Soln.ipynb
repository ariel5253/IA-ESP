{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    },
    "toc-autonumbering": false
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "daeb97ef-d44f-48b1-91ea-54d03c25c95f",
      "cell_type": "markdown",
      "source": "# Ejercicio: Ingeniería de X entradas (características) y Regresión Polinómica\n",
      "metadata": {}
    },
    {
      "id": "c0439826-ecd0-4a53-a1a8-1d0d9505f601",
      "cell_type": "markdown",
      "source": "## Objetivos\nEn este ejercicio usted:\n- explorará la ingeniería de X entradas (características) y la regresión polinómica, lo que le permite usar la maquinaria de la regresión lineal para ajustar funciones muy complicadas, incluso muy no lineales.\n",
      "metadata": {}
    },
    {
      "id": "b98f0bec-4400-4895-8c7d-97967e0751db",
      "cell_type": "markdown",
      "source": "## Herramientas\nUtilizará la función desarrollada en ejercicios anteriores así como matplotlib y NumPy. ",
      "metadata": {}
    },
    {
      "id": "968d68dd-3ba2-4d86-a237-0e3d1d63df2b",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom ejerc_utils_multi import zscore_normalize_features, run_gradient_descent_feng\nnp.set_printoptions(precision=2)  # precisión de visualización reducida en arreglos numpy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "979bdec4-4714-4c21-b945-124384bb1f2a",
      "cell_type": "markdown",
      "source": "<a name='FeatureEng'></a>\n# Visión General de Ingeniería de X entradas (características) y Regresión Polinómica\n\nDe forma predeterminada, la regresión lineal proporciona un medio para construir modelos de la forma:\n$$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$ \n¿Qué pasa si sus X entradas (características)/datos son no lineales o son combinaciones de X entradas (características)? Por ejemplo, los precios de viviendas no tienden a ser lineales con el área habitable (metros) sino que penalizan casas muy pequeñas o muy grandes, resultando en las curvas mostradas en la imagen de arriba. ¿Cómo podemos usar la maquinaria de la regresión lineal para ajustar esta curva? Recuerde, la 'maquinaria' que tenemos es la capacidad de modificar los parámetros $\\mathbf{w}$, $\\mathbf{b}$ en (1) para 'ajustar' la ecuación a los datos de entrenamiento. Sin embargo, ningún ajuste de $\\mathbf{w}$,$\\mathbf{b}$ en (1) logrará un ajuste a una curva no lineal.\n",
      "metadata": {}
    },
    {
      "id": "34c0e457-e6f7-4bbf-b9f3-a4d6f017161d",
      "cell_type": "markdown",
      "source": "<a name='PolynomialFeatures'></a>\n## X entradas (características) polinómicas\n\nArriba estábamos considerando un escenario donde los datos eran no lineales. Probemos usando lo que sabemos hasta ahora para ajustar una curva no lineal. Comenzaremos con una cuadrática simple: $y = 1+x^2$\n\nUsted ya está familiarizado con todas las rutinas que estamos usando. Están disponibles en el archivo lab_utils.py para revisión. Usaremos [`np.c_[..]`](https://numpy.org/doc/stable/reference/generated/numpy.c_.html) que es una rutina de NumPy para concatenar a lo largo del límite de columna.",
      "metadata": {}
    },
    {
      "id": "2e2f16ec-ad15-4711-94f8-2a5f1b10ec06",
      "cell_type": "code",
      "source": "# crear datos objetivo\nx = np.arange(0, 20, 1)\ny = 1 + x**2\nX = x.reshape(-1, 1)\n\nmodel_w,model_b = run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-2)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Valor real\"); plt.title(\"sin ingeniería de X entradas (características)\")\nplt.plot(x,X@model_w + model_b, label=\"Valor predicho\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35f0091d-8699-487f-8fbc-740204eaa37e",
      "cell_type": "markdown",
      "source": "Bien, como era de esperar, no es un buen ajuste. Lo que se necesita es algo como $y= w_0x_0^2 + b$, o una **X entrada (característica) polinómica**.\nPara lograr esto, puede modificar los *datos de entrada* para *ingenierizar* las X entradas (características) necesarias. Si reemplaza los datos originales por una versión que eleva al cuadrado el valor de $x$, entonces puede lograr $y= w_0x_0^2 + b$. Probémoslo. Reemplace `X` por `X**2` abajo:",
      "metadata": {}
    },
    {
      "id": "4bc73efd-c95f-4453-ac93-ddb430d281f9",
      "cell_type": "code",
      "source": "# crear datos objetivo\nx = np.arange(0, 20, 1)\ny = 1 + x**2\n\n# Ingeniería de X entradas (características) \nX = x**2      #<-- X entrada (característica) ingenierizada agregada",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2def0e6f-0892-4e5b-acc3-35ce40f7ef35",
      "cell_type": "code",
      "source": "X = X.reshape(-1, 1)  #X debe ser una matriz 2-D\nmodel_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha = 1e-5)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Valor real\"); plt.title(\"X entrada (característica) x**2 agregada\")\nplt.plot(x, np.dot(X,model_w) + model_b, label=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "733b4fd1-1bce-4cde-9add-9b96075326f9",
      "cell_type": "markdown",
      "source": "¡Genial! ajuste casi perfecto. Observe los valores de $\\mathbf{w}$ y b impresos justo arriba del gráfico: `w,b encontrados por descenso de gradiente: w: [1.], b: 0.0490`. El descenso de gradiente modificó nuestros valores iniciales de $\\mathbf{w},b $ a (1.0,0.049) o un modelo de $y=1*x_0^2+0.049$, muy cerca de nuestro objetivo de $y=1*x_0^2+1$. Si lo ejecuta por más tiempo, podría ser una mejor coincidencia. ",
      "metadata": {}
    },
    {
      "id": "ea14af2c-6bab-4650-a9ce-9baeb16f1e7b",
      "cell_type": "markdown",
      "source": "### Selección de X entradas (características)\n<a name='GDF'></a>\nArriba, sabíamos que se requería un término $x^2$. No siempre es obvio qué X entradas (características) se requieren. Se podrían agregar varias X entradas (características) potenciales para intentar encontrar las más útiles. Por ejemplo, ¿qué pasaría si en su lugar intentáramos: $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ ? \n\nEjecute las siguientes celdas. ",
      "metadata": {}
    },
    {
      "id": "2aa1d77f-036e-493d-a5a5-c902cd78121e",
      "cell_type": "code",
      "source": "# crear datos objetivo\nx = np.arange(0, 20, 1)\ny = x**2\n\n# ingeniería de X entradas (características) .\nX = np.c_[x, x**2, x**3]   #<-- X entrada (característica) ingenierizada agregada",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f2768ef8-aeb1-4081-9c6c-e540bc4a57b0",
      "cell_type": "code",
      "source": "model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-7)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Valor real\"); plt.title(\"X entradas (características) x, x**2, x**3\")\nplt.plot(x, X@model_w + model_b, label=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cdcdd790-78a8-4666-b77c-c2fb81f5b2dc",
      "cell_type": "markdown",
      "source": "Observe el valor de $\\mathbf{w}$, `[0.08 0.54 0.03]` y b es `0.0106`. Esto implica que el modelo después de ajustar/entrenar es:\n$$ 0.08x + 0.54x^2 + 0.03x^3 + 0.0106 $$\nEl descenso de gradiente ha enfatizado los datos que mejor se ajustan a los datos de $x^2$ aumentando el término $w_1$ en relación con los demás. Si lo ejecutara durante mucho tiempo, continuaría reduciendo el impacto de los otros términos. \n\n>El descenso de gradiente está eligiendo las X entradas (características) 'correctas' para nosotros al enfatizar su parámetro asociado\n\nRevisemos esta idea:\n- un valor de peso menor implica una X entrada (característica) menos importante/correcta, y en el extremo, cuando el peso se vuelve cero o muy cercano a cero, la X entrada (característica) asociada no es útil para ajustar el modelo a los datos.\n- arriba, después de ajustar, el peso asociado a la X entrada (característica) $x^2$ es mucho mayor que los pesos para $x$ o $x^3$ ya que es la más útil para ajustar los datos. ",
      "metadata": {}
    },
    {
      "id": "f18deb44-450e-46dc-b03c-acc0fc567591",
      "cell_type": "markdown",
      "source": "### Una Vista Alternativa\nArriba, las X entradas (características) polinómicas se eligieron en función de qué tan bien coincidían con los datos objetivo. Otra forma de pensar en esto es notar que seguimos usando regresión lineal una vez que hemos creado nuevas X entradas (características). Dado eso, las mejores X entradas (características) serán lineales respecto al objetivo. Esto se entiende mejor con un ejemplo. ",
      "metadata": {}
    },
    {
      "id": "a22a6725-c905-4ad7-bfe1-a4fc63d1a1ba",
      "cell_type": "code",
      "source": "# crear datos objetivo\nx = np.arange(0, 20, 1)\ny = x**2\n\n# ingeniería de X entradas (características) .\nX = np.c_[x, x**2, x**3]   #<-- X entrada (característica) ingenierizada agregada\nX_features = ['x','x^2','x^3']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d63bac2a-2a30-49aa-aae1-cb9c3bcdce2c",
      "cell_type": "code",
      "source": "fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(X[:,i],y)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"y\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "918d5732-622b-4223-83f7-ecdbf1f93405",
      "cell_type": "markdown",
      "source": "Arriba, es claro que la X entrada (característica) $x^2$ mapeada contra el valor objetivo $y$ es lineal. La regresión lineal puede entonces generar fácilmente un modelo usando esa X entrada (característica).",
      "metadata": {}
    },
    {
      "id": "c374385c-aad2-4146-91ef-a78b12b140ee",
      "cell_type": "markdown",
      "source": "### Escalado de X entradas (características)\nComo se describió en el ejercicio anterior, si el conjunto de datos tiene X entradas (características) con escalas significativamente diferentes, se debe aplicar escalado de X entradas (características) para acelerar el descenso de gradiente. En el ejemplo anterior, hay $x$, $x^2$ y $x^3$ que naturalmente tendrán escalas muy diferentes. Apliquemos la normalización z-score a nuestro ejemplo.",
      "metadata": {}
    },
    {
      "id": "28334e31-1074-49c5-8882-95b805174730",
      "cell_type": "code",
      "source": "# crear datos objetivo\nx = np.arange(0,20,1)\nX = np.c_[x, x**2, x**3]\nprint(f\"Rango pico a pico por columna en X original: {np.ptp(X,axis=0)}\")\n\n# agregar normalización por la media \nX = zscore_normalize_features(X)     \nprint(f\"Rango pico a pico por columna en X normalizado: {np.ptp(X,axis=0)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "802b9ab4-5249-4b9b-be27-54317405bfaa",
      "cell_type": "markdown",
      "source": "Ahora podemos intentar de nuevo con un valor de alpha más agresivo:",
      "metadata": {}
    },
    {
      "id": "260f4ea8-809d-4189-98db-de34b0ef4ca5",
      "cell_type": "code",
      "source": "x = np.arange(0,20,1)\ny = x**2\n\nX = np.c_[x, x**2, x**3]\nX = zscore_normalize_features(X) # features = características (X entradas)\n\nmodel_w, model_b = run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Valor real\"); plt.title(\"X entradas (características) normalizadas x x**2, x**3\")\nplt.plot(x,X@model_w + model_b, label=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f1414f73-326e-4b69-a28a-07ebdd130839",
      "cell_type": "markdown",
      "source": "El escalado de X entradas (características) permite que esto converja mucho más rápido.   \nObserve nuevamente los valores de $\\mathbf{w}$. El término $w_1$, que es el término $x^2$, es el más enfatizado. El descenso de gradiente prácticamente ha eliminado el término $x^3$.",
      "metadata": {}
    },
    {
      "id": "79f1e8c0-ecad-41f7-b59f-a9efec2f51ce",
      "cell_type": "markdown",
      "source": "### Funciones Complejas\nCon la ingeniería de X entradas (características), incluso funciones bastante complejas pueden ser modeladas:",
      "metadata": {}
    },
    {
      "id": "36f11d15-6844-4812-b33b-34aa4c1d7b90",
      "cell_type": "code",
      "source": "x = np.arange(0,20,1)\ny = np.cos(x/2)\n\nX = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]\nX = zscore_normalize_features(X) \n\nmodel_w,model_b = run_gradient_descent_feng(X, y, iterations=1000000, alpha = 1e-1)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Valor real\"); plt.title(\"X entradas (características) normalizadas x x**2, x**3\")\nplt.plot(x,X@model_w + model_b, label=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6f6db0ef-869a-4bdb-b942-a5e421377218",
      "cell_type": "markdown",
      "source": "\n## ¡Felicitaciones!\nEn este ejercicio usted:\n- aprendió cómo la regresión lineal puede modelar funciones complejas, incluso altamente no lineales, usando ingeniería de X entradas (características)\n- reconoció que es importante aplicar escalado de características al hacer ingeniería de X entradas (características)",
      "metadata": {}
    },
    {
      "id": "f2c6843e-51d7-44ca-9321-5d46d445b25b",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}