{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    },
    "dl_toc_settings": {
      "rndtag": "15456"
    },
    "toc-autonumbering": false
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "fa5bfc04-c7c2-4b9c-9060-7a2bdb21059c",
      "cell_type": "markdown",
      "source": "# Ejercicio: Regresión Lineal con Múltiples Variables\n\nEn este ejercicio, extenderá las estructuras de datos y las rutinas desarrolladas previamente para soportar múltiples características. Varias rutinas se actualizan haciendo que el ejercicio parezca extenso, pero solo realiza pequeños ajustes a las rutinas anteriores, por lo que es rápido de revisar.\n# Contenido\n- [&nbsp;&nbsp;1.1 Objetivos](#toc_15456_1.1)\n- [&nbsp;&nbsp;1.2 Herramientas](#toc_15456_1.2)\n- [&nbsp;&nbsp;1.3 Notación](#toc_15456_1.3)\n- [2 Enunciado del Problema](#toc_15456_2)\n- [&nbsp;&nbsp;2.1 Matriz X que contiene nuestros ejemplos](#toc_15456_2.1)\n- [&nbsp;&nbsp;2.2 Vector de parámetros w, b](#toc_15456_2.2)\n- [3 Predicción del Modelo con Múltiples Variables](#toc_15456_3)\n- [&nbsp;&nbsp;3.1 Predicción individual elemento por elemento](#toc_15456_3.1)\n- [&nbsp;&nbsp;3.2 Predicción individual, vector](#toc_15456_3.2)\n- [4 Cálculo del Costo con Múltiples Variables](#toc_15456_4)\n- [5 Descenso de Gradiente con Múltiples Variables](#toc_15456_5)\n- [&nbsp;&nbsp;5.1 Calcular el Gradiente con Múltiples Variables](#toc_15456_5.1)\n- [&nbsp;&nbsp;5.2 Descenso de Gradiente con Múltiples Variables](#toc_15456_5.2)\n- [6 ¡Felicitaciones!](#toc_15456_6)\n",
      "metadata": {}
    },
    {
      "id": "d24c11e4-5c00-4f06-a413-5e0743c290aa",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_1.1\"></a>\n## 1.1 Objetivos\n- Extender nuestras rutinas de regresión para soportar múltiples características\n    - Extender las estructuras de datos para soportar múltiples características\n    - Reescribir las rutinas de predicción, costo y gradiente para soportar múltiples características\n    - Utilizar NumPy `np.dot` para vectorizar sus implementaciones para mayor velocidad y simplicidad",
      "metadata": {}
    },
    {
      "id": "cc91a99a-8b7b-4303-8467-845e2cf25e69",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_1.2\"></a>\n## 1.2 Herramientas\nEn este ejercicio, utilizaremos: \n- NumPy, una biblioteca popular para computación científica\n- Matplotlib, una biblioteca popular para graficar datos",
      "metadata": {}
    },
    {
      "id": "2564c6d8-5889-49fa-8edd-1e8ce574cbdc",
      "cell_type": "code",
      "source": "import copy, math\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nnp.set_printoptions(precision=2)  # precisión de visualización reducida en arreglos numpy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a1ef4861-154a-43db-9855-96513fbd8f6d",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_1.3\"></a>\n## 1.3 Notación\nAquí hay un resumen de parte de la notación que encontrará, actualizada para múltiples características.\n\n| Notación | Descripción | Python (si aplica) |\n|:---------|:------------------------------------------------------------|:---------------------|\n| $a$ | escalar, no en negrita | |\n| $\\mathbf{a}$ | vector, en negrita | |\n| $\\mathbf{A}$ | matriz, mayúscula en negrita | |\n| **Regresión** | | |\n| $\\mathbf{X}$ | matriz de ejemplos de entrenamiento | `X_train` |\n| $\\mathbf{y}$ | objetivos de ejemplos de entrenamiento | `y_train` |\n| $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i$-ésimo ejemplo de entrenamiento | `X[i]`, `y[i]` |\n| $m$ | número de ejemplos de entrenamiento | `m` |\n| $n$ | número de características en cada ejemplo | `n` |\n| $\\mathbf{w}$ | parámetro: peso | `w` |\n| $b$ | parámetro: sesgo | `b` |\n| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | Resultado de la evaluación del modelo en $\\mathbf{x}^{(i)}$ parametrizado por $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$ | `f_wb` |\n",
      "metadata": {}
    },
    {
      "id": "f1d27c23-6a0d-4866-a435-cf44db09769f",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_2\"></a>\n# 2 Enunciado del Problema\n\nUtilizará el ejemplo motivador de predicción de precios de viviendas. El conjunto de datos de entrenamiento contiene tres ejemplos con cuatro características (tamaño, habitaciones, pisos y antigüedad) mostrados en la tabla a continuación. Tenga en cuenta que, a diferencia de los ejercicios anteriores, el tamaño está en pies cuadrados y no en miles de pies cuadrados. Esto causa un problema que resolverá en el próximo ejercicio.\n\n| Tamaño (metros²) | Número de Habitaciones | Número de Pisos | Antigüedad de la Casa | Precio (miles de dólares) |\n| -------------- | --------------------- | --------------- | --------------------- | ------------------------- |\n| 2104           | 5                     | 1               | 45                    | 460                      |\n| 1416           | 3                     | 2               | 40                    | 232                      |\n| 852            | 2                     | 1               | 35                    | 178                      |\n\nConstruirá un modelo de regresión lineal usando estos valores para luego predecir el precio de otras casas. Por ejemplo, una casa de 1200 pies², 3 habitaciones, 1 piso, 40 años de antigüedad.\n\nPor favor, ejecute la siguiente celda de código para crear sus variables `X_train` y `y_train`.",
      "metadata": {}
    },
    {
      "id": "99149c71-784c-4611-ad8f-55f75a32549a",
      "cell_type": "code",
      "source": "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\ny_train = np.array([460, 232, 178])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0fe134e7-86a7-4b00-a2c9-189a184d9ec9",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_2.1\"></a>\n## 2.1 Matriz X que contiene nuestros ejemplos\nSimilar a la tabla anterior, los ejemplos se almacenan en una matriz NumPy `X_train`. Cada fila de la matriz representa un ejemplo. Cuando tiene $m$ ejemplos de entrenamiento ($m$ es tres en nuestro ejemplo), y hay $n$ características (cuatro en nuestro ejemplo), $\\mathbf{X}$ es una matriz con dimensiones ($m$, $n$) (m filas, n columnas).\n\n\n$$\\mathbf{X} = \n\\begin{pmatrix}\n x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n \\cdots \\\\\n x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n\\end{pmatrix}\n$$\nnotación:\n- $\\mathbf{x}^{(i)}$ es el vector que contiene el ejemplo i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n- $x^{(i)}_j$ es el elemento j en el ejemplo i. El superíndice entre paréntesis indica el número de ejemplo mientras que el subíndice representa un elemento.  \n\nMuestre los datos de entrada.",
      "metadata": {}
    },
    {
      "id": "669ce2d4-7703-4c09-800f-2f24c6f05d8b",
      "cell_type": "code",
      "source": "# los datos se almacenan en un arreglo/matriz numpy\nprint(f\"Forma de x (shape): {X_train.shape}, Tipo de X (type):{type(X_train)})\")\nprint(X_train)\nprint(f\"Forma de y (shape): {y_train.shape}, Tipo de y (type):{type(y_train)})\")\nprint(y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "64990172-16f3-4d04-99f5-54d07f38c1d8",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_2.2\"></a>\n## 2.2 Vector de parámetros w, b\n\n* $\\mathbf{w}$ es un vector con $n$ elementos.\n  - Cada elemento contiene el parámetro asociado a una característica.\n  - en nuestro conjunto de datos, n es 4.\n  - conceptualmente, lo representamos como un vector columna\n\n$$\\mathbf{w} = \\begin{pmatrix}\nw_0 \\\\ \nw_1 \\\\\n\\cdots\\\\\nw_{n-1}\n\\end{pmatrix}\n$$\n* $b$ es un parámetro escalar.  ",
      "metadata": {}
    },
    {
      "id": "5c282b55-0aba-44b5-b886-b206fbdab06e",
      "cell_type": "markdown",
      "source": "Para la demostración, $\\mathbf{w}$ y $b$ se cargarán con algunos valores iniciales seleccionados que están cerca del óptimo. $\\mathbf{w}$ es un vector NumPy 1-D.",
      "metadata": {}
    },
    {
      "id": "3f33b9b2-2aa0-462c-907d-a48607b896bd",
      "cell_type": "code",
      "source": "b_init = 785.1811367994083 # valor inicial de sesgo\nw_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618]) # valor inicial de los pesos\nprint(f\"Forma de w_init (shape): {w_init.shape}, typo de b_init (type): {type(b_init)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b2f496e2-eba2-44e1-b444-6569d03fda76",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_3\"></a>\n# 3 Predicción del Modelo con Múltiples Variables\nLa predicción del modelo con múltiples variables está dada por el modelo lineal:\n\n$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\no en notación vectorial:\n$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \ndonde $\\cdot$ es un `producto punto` de vectores\nPara demostrar el producto punto, implementaremos la predicción usando (1) y (2).",
      "metadata": {}
    },
    {
      "id": "10818af3-8dc6-45f7-a67f-cab6d7fe8c8c",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_3.1\"></a>\n## 3.1 Predicción individual elemento por elemento\nNuestra predicción anterior multiplicaba un valor de característica por un parámetro y sumaba un parámetro de sesgo. Una extensión directa de nuestra implementación previa de predicción a múltiples características sería implementar (1) anterior usando un bucle sobre cada elemento, realizando la multiplicación con su parámetro y luego sumando el parámetro de sesgo al final.\n",
      "metadata": {}
    },
    {
      "id": "379e452d-50b6-47a0-9a58-b04b9b33b860",
      "cell_type": "code",
      "source": "def predict_single_loop(x, w, b): \n    \"\"\"\n    predicción individual usando regresión lineal\n    \n    Argumentos:\n      x (ndarray): Forma/shape (n,) ejemplo con múltiples características\n      w (ndarray): Forma/shape (n,) parámetros del modelo    \n      b (escalar):  parámetro del modelo     \n      \n    Retorna:\n      p (escalar):  predicción\n    \"\"\"\n    n = x.shape[0]\n    p = 0\n    for i in range(n):\n        p_i = x[i] * w[i]  \n        p = p + p_i         \n    p = p + b                \n    return p",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "23aae9dc-4649-49bb-8392-14076c0c294e",
      "cell_type": "code",
      "source": "# obtener una fila de nuestros datos de entrenamiento\nx_vec = X_train[0,:]\nprint(f\"Forma de x_vec (shape) {x_vec.shape}, Valor de x_vec: {x_vec}\")\n\n# hacer una predicción\nf_wb = predict_single_loop(x_vec, w_init, b_init)\nprint(f\"Forma de f_wb (shape) {f_wb.shape}, predicción: {f_wb}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4ce82e9b-15c7-4d5d-9fb3-b48fdd6d555a",
      "cell_type": "markdown",
      "source": "Nota la forma (shape) de `x_vec`. Es un vector NumPy 1-D con 4 elementos, (4,). El resultado, `f_wb` es un escalar.",
      "metadata": {}
    },
    {
      "id": "75379941-9c6f-4bbd-adfe-a5addd7c8ecb",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_3.2\"></a>\n## 3.2 Predicción individual, vector\n\nObservando que la ecuación (1) anterior puede implementarse usando el producto punto como en (2) anterior. Podemos utilizar operaciones vectoriales para acelerar las predicciones.\n\nRecuerde del ejercicio de Python/Numpy que NumPy `np.dot()`[[enlace](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] puede usarse para realizar un producto punto de vectores. ",
      "metadata": {}
    },
    {
      "id": "0b97ec52-a0cc-4a68-a9e0-79a862bb3416",
      "cell_type": "code",
      "source": "def predict(x, w, b): \n    \"\"\"\n    predicción individual usando regresión lineal\n    Argumentos:\n      x (ndarray): Forma/shape (n,) ejemplo con múltiples características\n      w (ndarray): Forma/shape (n,) parámetros del modelo   \n      b (escalar):             parámetro del modelo \n      \n    Retorna:\n      p (escalar):  predicción\n    \"\"\"\n    p = np.dot(x, w) + b     \n    return p    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1c2ca0a0-7bc0-4071-98cd-b65b455496be",
      "cell_type": "code",
      "source": "# obtener una fila de nuestros datos de entrenamiento\nx_vec = X_train[0,:]\nprint(f\"Forma de x_vec (shape) {x_vec.shape}, valor de x_vec: {x_vec}\")\n\n# hacer una predicción\nf_wb = predict(x_vec,w_init, b_init)\nprint(f\"Forma de f_wb (shape) {f_wb.shape}, predicción: {f_wb}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b515a2fa-dc5c-4bcd-8c48-8b52741f45e7",
      "cell_type": "markdown",
      "source": "Los resultados y formas son los mismos que la versión anterior que usaba bucles. De ahora en adelante, se usará `np.dot` para estas operaciones. La predicción ahora es una sola instrucción. La mayoría de las rutinas la implementarán directamente en lugar de llamar a una rutina de predicción separada.",
      "metadata": {}
    },
    {
      "id": "9cf73d3c-d698-4f16-9f06-cd526b607914",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_4\"></a>\n# 4 Cálculo del Costo con Múltiples Variables\nLa ecuación para la función de costo con múltiples variables $J(\\mathbf{w},b)$ es:\n$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \ndonde:\n$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n\n\nA diferencia de ejercicios anteriores, $\\mathbf{w}$ y $\\mathbf{x}^{(i)}$ son vectores en lugar de escalares, soportando múltiples características.",
      "metadata": {}
    },
    {
      "id": "68e7234f-0e98-4fe0-b413-1acbbb68f83d",
      "cell_type": "markdown",
      "source": "A continuación se muestra una implementación de las ecuaciones (3) y (4). Tenga en cuenta que esto utiliza un *patrón estándar para este curso* donde se utiliza un bucle for sobre todos los ejemplos `m`.",
      "metadata": {}
    },
    {
      "id": "cdeaf7ad-c89e-4d26-a16d-9044e6447f09",
      "cell_type": "code",
      "source": "def compute_cost(X, y, w, b): \n    \"\"\"\n    calcular el costo\n    Argumentos:\n      X (ndarray (m,n)): Datos, m ejemplos con n características\n      y (ndarray (m,)) : valores objetivo\n      w (ndarray (n,)) : parámetros del modelo  \n      b (escalar)      : parámetro del modelo\n      \n    Retorna:\n      cost (escalar): costo\n    \"\"\"\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):                                \n        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = escalar (ver np.dot)\n        cost = cost + (f_wb_i - y[i])**2       #escalar\n    cost = cost / (2 * m)                      #escalar    \n    return cost",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5fcd55e0-4fd3-49ea-aaf7-11f8c618952d",
      "cell_type": "code",
      "source": "# Calcular y mostrar el costo usando nuestros parámetros óptimos preseleccionados. \ncost = compute_cost(X_train, y_train, w_init, b_init)\nprint(f'Costo con un w óptimo: {cost}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "754cae65-8835-4e88-b5a7-889cd01239a9",
      "cell_type": "markdown",
      "source": "**Resultado esperado**: Cost at optimal w : 1.5578904045996674e-12",
      "metadata": {}
    },
    {
      "id": "9a80acf7-92d5-4654-b76f-578f459ca018",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_5\"></a>\n# 5 Descenso de Gradiente con Múltiples Variables\nEl descenso de gradiente para múltiples variables:\n\n$$\\begin{align*} \\text{repetir}&\\text{ hasta convergencia:} \\; \\lbrace \\newline\\;\n& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{para j = 0..n-1}\\newline\n&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}$$\n\ndonde, n es el número de características, los parámetros $w_j$,  $b$, se actualizan simultáneamente y donde  \n\n$$\n\\begin{align}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n\\end{align}\n$$\n* m es el número de ejemplos de entrenamiento en el conjunto de datos\n\n    \n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ es la predicción del modelo, mientras que $y^{(i)}$ es el valor objetivo\n",
      "metadata": {}
    },
    {
      "id": "b7cc1bb0-341f-4602-8fff-4be72e1f0cd8",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_5.1\"></a>\n## 5.1 Calcular el Gradiente con Múltiples Variables\nA continuación se muestra una implementación para calcular las ecuaciones (6) y (7). Hay muchas formas de implementarlo. En esta versión, hay un\n- bucle externo sobre todos los ejemplos m. \n    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ para el ejemplo puede calcularse directamente y acumularse\n    - en un segundo bucle sobre todas las características n:\n        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ se calcula para cada $w_j$.\n   ",
      "metadata": {}
    },
    {
      "id": "c5dade17-757d-45ff-b18b-66a8d23b594e",
      "cell_type": "code",
      "source": "def compute_gradient(X, y, w, b): \n    \"\"\"\n    Calcula el gradiente para regresión lineal \n    Argumentos:\n      X (ndarray (m,n)): Datos, m ejemplos con n características\n      y (ndarray (m,)) : valores objetivo\n      w (ndarray (n,)) : parámetros del modelo  \n      b (escalar)      : parámetro del modelo\n      \n    Retorna:\n      dj_dw (ndarray (n,)): El gradiente del costo respecto a los parámetros w. \n      dj_db (escalar):       El gradiente del costo respecto al parámetro b. \n    \"\"\"\n    m,n = X.shape           #(número de ejemplos, número de características)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]   \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw / m                                \n    dj_db = dj_db / m                                \n        \n    return dj_db, dj_dw",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "163ba81a-5240-4083-812f-68060cb7584f",
      "cell_type": "code",
      "source": "#Calcular y mostrar el gradiente \ntmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\nprint(f'dj_db con w,b inicial: {tmp_dj_db}')\nprint(f'dj_dw con w,b inicial: \\n {tmp_dj_dw}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5bda9021-8c61-425d-9a70-eebb1256fadf",
      "cell_type": "markdown",
      "source": "**Resultado esperado**:   \ndj_db at initial w,b: -1.6739251122999121e-06  \ndj_dw at initial w,b:   \n [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  ",
      "metadata": {}
    },
    {
      "id": "5a2244a4-0920-4529-a072-b38e2f50c9b9",
      "cell_type": "markdown",
      "source": "<a name=\"toc_15456_5.2\"></a>\n## 5.2 Descenso de Gradiente con Múltiples Variables\nLa siguiente rutina implementa la ecuación (5) anterior.",
      "metadata": {}
    },
    {
      "id": "3eb01d29-c027-4d49-b229-b05ec9a2cbc4",
      "cell_type": "code",
      "source": "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n    \"\"\"\n    Realiza descenso de gradiente por lotes para aprender w y b. Actualiza w y b tomando \n    num_iters pasos de gradiente con tasa de aprendizaje alpha\n    \n    Argumentos:\n      X (ndarray (m,n))   : Datos, m ejemplos con n características\n      y (ndarray (m,))    : valores objetivo\n      w_in (ndarray (n,)) : parámetros iniciales del modelo  \n      b_in (escalar)      : parámetro inicial del modelo\n      cost_function       : función para calcular el costo\n      gradient_function   : función para calcular el gradiente\n      alpha (float)       : Tasa de aprendizaje\n      num_iters (int)     : número de iteraciones para ejecutar descenso de gradiente\n      \n    Retorna:\n      w (ndarray (n,)) : Valores actualizados de los parámetros \n      b (escalar)      : Valor actualizado del parámetro \n    \"\"\"\n    \n    # Un arreglo para almacenar el costo J y los w en cada iteración, principalmente para graficar después\n    J_history = []\n    w = copy.deepcopy(w_in)  #evitar modificar w global dentro de la función\n    b = b_in\n    \n    for i in range(num_iters):\n\n        # Calcular el gradiente y actualizar los parámetros\n        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n\n        # Actualizar parámetros usando w, b, alpha y gradiente\n        w = w - alpha * dj_dw               ##None\n        b = b - alpha * dj_db               ##None\n      \n        # Guardar el costo J en cada iteración\n        if i<100000:      # prevenir agotamiento de recursos \n            J_history.append( cost_function(X, y, w, b))\n\n        # Imprimir el costo cada cierto intervalo, 10 veces o tantas iteraciones si < 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n        \n    return w, b, J_history #retorna w,b final y el historial de J para graficar",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5167f4b5-f2c3-42c8-b9cf-59108f9779ce",
      "cell_type": "markdown",
      "source": "En la siguiente celda probará la implementación. ",
      "metadata": {}
    },
    {
      "id": "207c53f8-3c14-483d-b41d-e9aaeb669bba",
      "cell_type": "code",
      "source": "# inicializar parámetros\ninitial_w = np.zeros_like(w_init)\ninitial_b = 0.\n# algunos ajustes para descenso de gradiente\niterations = 1000\nalpha = 5.0e-7\n# ejecutar descenso de gradiente \nw_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n                                                    compute_cost, compute_gradient, \n                                                    alpha, iterations)\nprint(f\"b,w encontrado por el descenso de gradiente: {b_final:0.2f},{w_final} \")\nm,_ = X_train.shape\nfor i in range(m):\n    print(f\"predicción: {np.dot(X_train[i], w_final) + b_final:0.2f}, valor objetivo: {y_train[i]}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "201084ca-10b4-4a3d-a32a-7ebec7017a63",
      "cell_type": "markdown",
      "source": "**Resultado esperado**:    \nb,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \nprediction: 426.19, target value: 460  \nprediction: 286.17, target value: 232  \nprediction: 171.47, target value: 178  ",
      "metadata": {}
    },
    {
      "id": "dac1209a-2e87-42b6-b42b-9abacb801d1e",
      "cell_type": "code",
      "source": "# graficar costo versus iteración  \nfig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\nax1.plot(J_hist)\nax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\nax1.set_title(\"Costo vs. iteración\");  ax2.set_title(\"Costo vs. iteración (final)\")\nax1.set_ylabel('Costo')             ;  ax2.set_ylabel('Costo') \nax1.set_xlabel('paso de iteración')   ;  ax2.set_xlabel('paso de iteración') \nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d08f0db4-e0b2-4c8b-9aaa-d4aaffcbc6d8",
      "cell_type": "markdown",
      "source": "*¡Estos resultados no son inspiradores*! El costo sigue disminuyendo y nuestras predicciones no son muy precisas. El próximo ejercicio explorará cómo mejorar esto.",
      "metadata": {}
    },
    {
      "id": "7784e061-0c5a-4690-b007-15ecd4feda19",
      "cell_type": "markdown",
      "source": "\n<a name=\"toc_15456_6\"></a>\n# 6 ¡Felicitaciones!\nEn este ejercicio usted:\n- Rediseñó las rutinas para regresión lineal, ahora con múltiples variables.\n- Utilizó NumPy `np.dot` para vectorizar las implementaciones",
      "metadata": {}
    }
  ]
}